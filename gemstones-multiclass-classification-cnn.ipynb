{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Context\nWhat is a gemstone? A gemstone (gem, fine gem, jewel, precious stone, or semi-precious stone) is a piece of mineral crystal which, in cut and polished form, is used to make jewelry or other adornments. Certain rocks (such as lapis lazuli and opal) and occasionally organic materials that are not minerals (such as amber, jet, pearl) are also used for jewelry and are therefore often considered to be gemstones as well (source: [wiki](https://en.wikipedia.org/wiki/Gemstone)).\n\nA few gemstones are used as gems in the crystal or other form in which they are found. Most however, are cut and polished for usage as jewelry. \n\n![](https://lh3.googleusercontent.com/proxy/oKyVDQ5e2whCQYHdmOhN6FopacGApwNz9NxAy2WRToIj3Bhs82Z7vY94GjwPueOCtgTKmgXCOhgHUJVEF6h3JS2EPBeI9Mus3TfIZZa6FhLOW6rqZY1ewMWXVKjYpo6Ixw)\n\nI have already collected the dataset of [gemstones images](https://www.kaggle.com/lsind18/gemstones-images) with 3.200+ images of faceted gems. Gemstones' images are grouped into 87 classes with division into train and test data in ratio ~ 0,9 : 0,1. The images are in various sizes of .jpg format. All gemstones are faceted in various shapes - round, oval, square, rectangle, heart. \n\n## todo:: Build a simple convolutional neural network from scratch over the dataset of [gemstones images](https://www.kaggle.com/lsind18/gemstones-images).\n### <font color='red'>If you like this notebook please upvote! Have fun</font> ðŸŒŸ   \n\n* Before we start the science, watch this short video to know how our beautiful nature forms gemstones:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nHTML('<center><iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/6YXKt0bMBJA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></center>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparation steps\n\n# I. Import necessary modules\n\n* `import os`: to read files from disk\n* `import cv2`: OpenCV is an image and video processing library  \n* `import matplotlib.pyplot as plt` and `import seaborn as sn`: to show images and graphs\n* `from random import randint`: to show random images\n* `import numpy as np`: linear algebra; all images will represent numpy-arrays.\n* other modules and their components will be imported later."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\nimport cv2\nfrom random import randint\n\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I. Check data \n\nFirst, check the number of files in every gemstone class in folder `/kaggle/input/gemstones-images`. Images are already divided into train (~2,800 images) and test (~400 images) data. Each class in train set contains `27 - 47` images, in test set - `4 - 6` images.\n* create list `CLASSES` which contains the names of 87 classes of gemstones based on folders names;\n* plot the distribution of data;\n* *uncomment block of code if you want a text output about each subfolder*."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"CLASSES, gems = [], [] # names of classes, count of images for each class\n\nfor root, dirs, files in os.walk('/kaggle/input/gemstones-images'):\n    f = os.path.basename(root)    # get class name - Amethyst, Onyx, etc    \n        \n    if len(files) > 0:\n        gems.append(len(files))\n        if f not in CLASSES:\n            CLASSES.append(f) # add folder name\n    \n    # uncomment this block if you want a text output about each subfolder\n    #count_dirs = 0\n    #for f in dirs:           # count subfolders\n        #count_dirs += 1\n    #depth = root.split(os.sep)\n    #print((len(depth) - 2) * '--'+'>', '{}:\\t {} folders, {} imgs'.format(os.path.basename(root), count_dirs, gems[-1] if gems!=[] else 0)) \n    \ngems_count = len(CLASSES) # 87 = number of classes\nprint('{} classes with {} images in total'.format(len(CLASSES), sum(gems)))\n\nf, ax = plt.subplots(figsize=(15,6))\nif(gems[0])<10:\n    plt.bar(range(gems_count), gems[gems_count:], label = 'Train data')\n    plt.bar(range(gems_count), gems[0:gems_count], label = 'Test data')\nelse:\n    plt.bar(range(gems_count), gems[0:gems_count], label = 'Train data')\n    plt.bar(range(gems_count), gems[gems_count:], label = 'Test data')\nax.grid()\nax.legend(fontsize = 12);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Orange small bars represent test data and blue bars represent train data. Data is normally distributed.\n\n# II. Prepare training data\n\n## 1. Prepare parameters\n\n* resize processed images to `img_w, img_h` - this option will be used when cropped and as a parameter of neural network; \n* provide train directory path."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"img_w, img_h = 220, 220    # width and height of image\ntrain_dir = '/kaggle/input/gemstones-images/train/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Create function which reads images and class names\n* this function will be also used with test images;\n* read each image from disk using `cv2` and resize it to `img_w*1.5, img_h*1.5`;\n* set `cv2.COLOR_BGR2RGB` option because opencv reads and displays an image as BGR color format instead of RGB color format. Without this option images will be shown in blue hue because `matplotlib` uses RGB to display image;\n* create a list of class names while reading folders - `Amethyst, Onyx, etc`;\n* when `Images` list is ready - convert it to Numpy array;\n* return tuple of 2 elements: Images and corresponding Labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_imgs_lbls(_dir):\n    Images, Labels = [], []\n    for root, dirs, files in os.walk(_dir):\n        f = os.path.basename(root)  # get class name - Amethyst, Onyx, etc       \n        for file in files:\n            Labels.append(f)\n            try:\n                image = cv2.imread(root+'/'+file)              # read the image (OpenCV)\n                image = cv2.resize(image,(int(img_w*1.5), int(img_h*1.5)))       # resize the image (images are different sizes)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converts an image from BGR color space to RGB\n                Images.append(image)\n            except Exception as e:\n                print(e)\n    Images = np.array(Images)\n    return (Images, Labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Create function which converts string labels to numbers\n* Convert string labels to a list of numbers using list `CLASSES`. The index will represent label of class, f.e. *Ruby = 0, Amethyst = 24*, etc.\n* when `Labels` list is ready - convert it to Numpy array."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_class_index(Labels):\n    for i, n in enumerate(Labels):\n        for j, k in enumerate(CLASSES):    # foreach CLASSES\n            if n == k:\n                Labels[i] = j\n    Labels = np.array(Labels)\n    return Labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Fill arrays of Images and corresponding Labels with data\n* Create two arrays `Train_Imgs, Train_Lbls` which contain images and corresponding names of classes of gemstones respectively;\n* Convert `Train_Lbls` with strings to list with corresponding numbers;\n* print the dimensions of both numpy arrays: `Train_Imgs` which stores pictures is 4-dimensional: **Number of images x Width of image x Height of image x Channel of image**."},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Imgs, Train_Lbls = read_imgs_lbls(train_dir)\nTrain_Lbls = get_class_index(Train_Lbls)\nprint('Shape of train images: {}'.format(Train_Imgs.shape))\nprint('Shape of train labels: {}'.format(Train_Lbls.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Plot images and their labels for preview\n* Using `matplotlib` and `random` show 16 (4x4) random images from the set and their labels (as string and as int number)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dim = 4 #you can change it;  4x4 dimension flat plot\n\nf,ax = plt.subplots(dim,dim) \nf.subplots_adjust(0,0,2,2)\nfor i in range(0,dim):\n    for j in range(0,dim):\n        rnd_number = randint(0,len(Train_Imgs))\n        cl = Train_Lbls[rnd_number]\n        ax[i,j].imshow(Train_Imgs[rnd_number])\n        ax[i,j].set_title(CLASSES[cl]+': ' + str(cl))\n        ax[i,j].axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From 4x4 plot above we can see that gemstones are mostly centered; but let's try to crop the edges.\n\n## 6. Crop edges of images using Canny algorithm\n> Canny is a popular edge detection algorithm, which detects the edges of objects present in an image.\n\n* Using `cv2.Canny` find the array representing frame which is the edges how the original picture will be cut;\n* Function `edge_and_cut(img)` receives single image and returns a **cropped image (`new_img`)** of the size `img_w, img_h`;\n* sometimes Canny algo cannot detect edges (f.e. when the object has almost same color as background) so array `edges` will be zero-valued. In this case use original image."},{"metadata":{"trusted":true},"cell_type":"code","source":"def edge_and_cut(img):\n    try:\n        edges = cv2.Canny(img, img_w, img_h)            \n        \n        if(np.count_nonzero(edges)>edges.size/10000):           \n            pts = np.argwhere(edges>0)\n            y1,x1 = pts.min(axis=0)\n            y2,x2 = pts.max(axis=0)\n            \n            new_img = img[y1:y2, x1:x2]           # crop the region\n            new_img = cv2.resize(new_img,(img_w, img_h))  # Convert back\n        else:\n            new_img = cv2.resize(img,(img_w, img_h))\n    \n    except Exception as e:\n        print(e)\n        new_img = cv2.resize(img,(img_w, img_h))\n    \n    return new_img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Show cropped images\n* Function `show_cropped` is kind-of duplicate `edge_and_cut()`: it shows same random examples of Canny algo work: **original image, Canny edges, image with bounding box, cropped image** for better understanding how Canny algo works."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def show_cropped(img):\n    emb_img = img.copy()\n    edges = cv2.Canny(img, img_w, img_h)\n    \n    if(np.count_nonzero(edges)>edges.size/10000):\n        pts = np.argwhere(edges>0)\n        y1,x1 = pts.min(axis=0)\n        y2,x2 = pts.max(axis=0)\n\n        new_img = img[y1:y2, x1:x2]  \n\n        edge_size = 1 #replace it with bigger size for larger images            \n\n        emb_img[y1-edge_size:y1+edge_size, x1:x2] = [255, 0, 0]\n        emb_img[y2-edge_size:y2+edge_size, x1:x2] = [255, 0, 0]\n        emb_img[y1:y2, x1-edge_size:x1+edge_size] = [255, 0, 0]\n        emb_img[y1:y2, x2-edge_size:x2+edge_size] = [255, 0, 0]\n\n        new_img = cv2.resize(new_img,(img_w, img_h))  # Convert to primary size  \n        \n    else:\n        new_img = cv2.resize(img,(img_w, img_h))\n    \n    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(10, 10))\n    ax[0].imshow(img, cmap='gray')\n    ax[0].set_title('Original Image', fontsize=14)\n    ax[1].imshow(edges, cmap='gray')\n    ax[1].set_title('Canny Edges', fontsize=14)\n    ax[2].imshow(emb_img, cmap='gray')\n    ax[2].set_title('Bounding Box', fontsize=14)       \n    ax[3].imshow(new_img, cmap='gray')\n    ax[3].set_title('Cropped', fontsize=14)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in range(0,3):\n    show_cropped(Train_Imgs[randint(0,len(Train_Imgs))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Replace train images with cropped images\n* Create function which calls `edge_and_cut` and replaces `Train_Imgs` numpy array with array of cropped images. Don't forget that images that cannot be cropped will be replced with originals;\n* Make sure the shape of final array is the same: NUMBER OF IMAGES x img_w x img_h x 3 (CHANNELS):"},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_images(Imgs):\n    CroppedImages = np.ndarray(shape=(len(Imgs), img_w, img_h, 3), dtype=np.int)\n\n    ind = 0\n    for im in Imgs: \n        x = edge_and_cut(im)\n        CroppedImages[ind] = x\n        ind += 1\n\n    return CroppedImages","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Imgs = crop_images(Train_Imgs)\nprint('Final shape of images in train set: {} '.format(Train_Imgs.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Split data into train and validation sets\n* use `sklearn` to split `Train_Imgs`, `Train_Lbls` into train (80%) and validation (20%) sets. **Important!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(Train_Imgs, Train_Lbls, shuffle = True, test_size = 0.2, random_state = 42)\nprint('Shape of X_train: {}, y_train: {} '.format(X_train.shape, y_train.shape))\nprint('Shape of X_val: {}, y_val: {} '.format(X_val.shape, y_val.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# III. Prepare for model creation\n## 1. Check devices\n\nUsing `tensorflow` check which devices uses the Kaggle platform.\n\n`XLA_CPU device`: CPU  \n`XLA_GPU device`: Tesla P100-PCIE-16GB (to accelerate computing use GPU mode).  \nXLA stands for *accelerated linear algebra*. It's Tensorflow's relatively optimizing compiler that can further speed up ML models.  \n\nRun this notebook with GPU mode: for example, using image size 190 x 190 and basic architecture of CNN mentioned above every epoch on CPU takes ~3 minutes, on GPU ~ 15 sec."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.client import device_lib\ndevices = device_lib.list_local_devices()\nprint(devices)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Import keras\nKeras is an open-source neural-network library written in Python which is capable of running on top of **TensorFlow**.\nFrom Keras needed:\n* `models` - type of models, import only `Sequential` \n* `layers` - layers corresponding to our model: as it a simple one take only `Conv2D`, `MaxPooling2D` and `AveragePooling2D`\n* `optimizers` - contains back propagation algorithms\n* `ImageDataGenerator` - for image augmenation (there are not so many samples of each class)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import optimizers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build a simple CNN  \nCNN (Convolutional neural network or ConvNet) is a class of deep neural networks, commonly applied to analyzing visual imagery. Here is the simpliest example of CNN with few layers using `Conv2D` - 2D convolution layer (spatial convolution over images) and `MaxPooling2D` - application of a moving window across a 2D input space.\n\n# I. Provide Hyperparameters\nHyperparameters are set before training; they represent the variables which determines the neural network structure and how the it is trained.  \n  \n## 1. Parameters for layers\n* Convolutional layer filter size (`filters`). The number of filters should depend on the complexity of dataset and the depth of neural network. A common setting to start with is [32, 64, 128] for three layers.  \n* `kernel_size` = number of filters  = a small window of pixels at a time (3Ã—3) which will be moved until the entire image is scanned. If images are smaller than 128Ã—128, work with smaller filters of 1Ã—1;\n* Width and Height of images were already provided. 2D convolutional layers take a three-dimensional input, typically an image with three color channels;\n* `max_pool` = max pooling is the application of a moving window across a 2D input space, where the maximum value within that window is the output: 2x2. "},{"metadata":{"trusted":true},"cell_type":"code","source":"filters = 32      # the dimensionality of the output space\nkernel_size = 3   # length of the 2D convolution window\nmax_pool = 2      # size of the max pooling windows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Parameters to fit the model\n* **epoch** describes the number of times the algorithm sees the ENTIRE dataset. Each time the algo has seen all samples in the dataset, an epoch has completed.  \n* since one epoch is too big to feed to the memory at once divide it in several smaller **batches**. Batch size is always factor of 2. \n* **Iterations** per epoch = number of passes, each pass using batch size number of examples.   \n\nSo if we have ~2200 (80%) training samples, and batch size is 32, then it will take ~70 iterations to complete 1 epoch."},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 70                                  # while testing you can change it\nbatch_size = 32                              # number of training samples using in each mini batch during GD (gradient descent) \niter_per_epoch = len(X_train) // batch_size  # each sample will be passed [iter_per_epoch] times during training\nval_per_epoch = len(X_val) // batch_size     # each sample will be passed [val_per_epoch] times during validation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# II. Provide a model\n\n## 1. Architect a model\nThe Sequential model is a linear stack of layers.\n* I use a kind of VGG network architecture:\n\n|   |       Layers       |\n|:-:|:------------------:|\n| 1 |  Conv2D 32 -> Pool |\n| 2 |  Conv2D 64 -> Pool |\n| 3 | Conv2D 128 -> Pool |\n| 4 | Conv2D 128 -> Pool |\n| 5 | Conv2D 128 -> Pool |\n| 6 |        FLAT        |\n| 7 |        Drop        |\n| 8 |      Dense 512     |\n| 9 | Dense len(CLASSES) |\n\n\n1. ADD 5 'blocks': \n   *  Conv2D with hypermarameters mantioned above: `Conv2D(kernel_size, (filters, filters), input_shape=(img_w, img_h, 3))` with activation function for each layer as a Rectified Linear Unit (ReLU): `Activation('relu')`  \n   * MaxPooling2D layer to reduce the spatial size of the incoming features; 2D input space: `MaxPooling2D(pool_size=(max_pool, max_pool))`  \n   * Do the same increading the kernel size: 32 -> 64 -> 128 -> 128 -> 128\n\n2. Flatten the input: transform the multidimensional vector into a single dimensional vector: `Flatten()`\n3. Add dropout layer which randomly sets a certain fraction of its input to 0 and helps to reduce overfitting: `Dropout(0.5)`\n5. Add fully connected layer with 512 nodes and activation function relu: `Dense(512), Activation('relu')`\n6. Provide last fully connected layer which specifies the number of classes of gemstones: **87**. `Softmax` activation function outputs a vector that represents the probability distributions of a list of potential outcomes: `Dense(87, activation='softmax')`   \n\n\n* Print the summary of the model."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"model = Sequential()\n\n# first layer\nmodel.add(Conv2D(batch_size, (kernel_size, kernel_size), activation='relu', padding='same', input_shape=(img_w, img_h, 3))) # 32\nmodel.add(MaxPooling2D((max_pool, max_pool))) #reduce the spatial size of incoming features\n\n# second layer\nmodel.add(Conv2D(2*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 64\nmodel.add(MaxPooling2D((max_pool, max_pool))) \n\n# third layer\nmodel.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\nmodel.add(MaxPooling2D((max_pool, max_pool))) \n\n# fourth layer\nmodel.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\nmodel.add(AveragePooling2D(pool_size= (2, 2), strides= (2, 2))) \n\n# fifth layer\nmodel.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\nmodel.add(MaxPooling2D((max_pool, max_pool))) \n\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16*batch_size, activation='relu'))                                             # 512\nmodel.add(Dense(87, activation='softmax'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model summary shows that there are more than 2M parameters to train and the information about different layers.\n\n## 2. Compile a model\n* Compile the model using `adam` optimizer which is a generalization of stochastic gradient descent (SGD) algo. Provided loss function is `sparse_categorical_crossentropy` as we are doing multiclass classification.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# III. Fit the train generator\n\n## 1. Image augmentation\n> **Image augmentation** is a creation of additional training data based on existing images, for example translation, rotation, flips and zoom.\n\n* As far as there are not so many samples for every class add a train data generator using class `ImageDataGenerator` with augmentation parameters. Using `ImageDataGenerator` class from Keras library create additional images of each gemstone class in the memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(              # this is the augmentation configuration used for training\n        rotation_range=25,\n        zoom_range=0.1,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        shear_range=0.2,\n        horizontal_flip=True\n        )\n\nval_datagen = ImageDataGenerator()                # for val/testing only rescaling function ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* the original image + examples of work of `ImageDataGenerator`: "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"n = randint(0,len(X_train))\nsamples = np.expand_dims(X_train[n], 0)\nit = train_datagen.flow(samples, batch_size=batch_size)\ncols = 7\n\nfig, ax = plt.subplots(nrows=1, ncols=cols, figsize=(15, 10))\nax[0].imshow(X_train[n], cmap='gray')\nax[0].set_title('Original', fontsize=10)\n\nfor i in range(1,cols):\n    batch = it.next()    # generate batch of images \n    image = batch[0].astype('uint32') # convert to unsigned int for viewing\n    ax[i].set_title('augmented {}'.format(i), fontsize=10)\n    ax[i].imshow(image, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create two numpy array iterators `train_gen` and `val_gen` and fill them with additional images:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = train_datagen.flow(X_train, y_train, batch_size=batch_size)\nval_gen = val_datagen.flow(X_val, y_val, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Fit the model\n* get a history object\n* If you see that `val_los` parameter is increasing that is *overfitting*. It happens when your model explains the training data too well, rather than picking up patterns that can help generalize over unseen data."},{"metadata":{"trusted":true},"cell_type":"code","source":"m = model.fit_generator(\n       train_gen,\n       steps_per_epoch= iter_per_epoch,\n       epochs=EPOCHS, \n       validation_data = val_gen,\n       validation_steps = val_per_epoch,\n       verbose = 1 # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n       )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ALMOST DONE!** ðŸ˜ˆ   \n\n## 3. Check the accuracy\n\n* plot the accuracy of model against size of epoch (train and val);\n* plot the loss of model against size of epoch (train and val)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\naxs[0].plot(m.history['accuracy'])\naxs[0].plot(m.history['val_accuracy'])\naxs[0].set_title('Model accuracy')\naxs[0].legend(['Train', 'Val'], loc='upper left')\n\naxs[1].plot(m.history['loss'])\naxs[1].plot(m.history['val_loss'])\naxs[1].set_title('Model loss')\naxs[1].legend(['Train', 'Val'], loc='upper left')\n\nfor ax in axs.flat:\n    ax.set(xlabel='Epoch')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* there is an overfitting: even though train and val accuracy are pretty close to each other, `val_loss` parameter often 'jumps'."},{"metadata":{},"cell_type":"markdown","source":"## 4. Score the model\n> Accuracy is a metric for evaluating classification models. `Accuracy  = Number of right predictions / Total number of predictions`.\n\n* Function `evaluate_generator` evaluates the model on a data generator: `score` is a list of scalars (loss and accuracy)."},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate_generator(val_gen, steps= len(val_gen))\n\nfor idx, metric in enumerate(model.metrics_names):\n    print('{}:{}'.format(metric, score[idx]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy of a model from scratch ~ 65%. Any suggestions on improving a model are taking up ðŸ˜Š"},{"metadata":{},"cell_type":"markdown","source":"## 5. Confusion matrix   \n> Confusion matrix can be pretty useful when evaluating multiclass classifications.   \n\n* `from sklearn.metrics import confusion_matrix`: Diagonal of matrix should be mostly filled with numbers."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_pre_test=model.predict(X_val)\ny_pre_test=np.argmax(y_pre_test,axis=1)\ncm=confusion_matrix(y_val,y_pre_test)\n\nplt.figure(figsize = (15,15))\nsn.heatmap(cm, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because of great amount of classes just **plot misclassified gemstones by model**. `numpy.argmax()` function returns the indices of maximum elements along the specific axis inside the array (`axis = 1` - 'horizontally').\n* Create a list of misclassified indexes which will be substitued into validation set `X_val`.  \n* Plot misclassified gemstones."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"x=(y_pre_test-y_val!=0).tolist()\nx=[i for i,l in enumerate(x) if l!=False]\n\nfig,ax=plt.subplots(1,5,sharey=False,figsize=(13,13))\nfig.tight_layout()\n\nfor i in range(5):\n    ax[i].imshow(X_val[x[i]][:,:,1])\n    ax[i].set_xlabel('{}, Pred: {}'.format(CLASSES[y_val[x[i]]],CLASSES[y_pre_test[x[i]]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Don't judge poor model. Just look at `Almandine`, `Garnet Red`, `Hessonite`, `Pyrope` and `Rhodolite`. Can you distinguish between them?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"red_stones = ['Almandine', 'Garnet Red', 'Hessonite', 'Pyrope', 'Rhodolite']\nred_stones = get_class_index(red_stones)\n\nfig,ax=plt.subplots(1,len(red_stones),sharey=False,figsize=(13,13))\nfig.tight_layout()\n\nfor i in range(len(red_stones)):\n    ax[i].imshow(Train_Imgs[np.where(Train_Lbls==red_stones[i])[0][1]])\n    ax[i].set_xlabel(CLASSES[red_stones[i]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Save the model\n* Save weights to reuse them instead of training again. Keras function `save` creates h5 file with weights. Use `new_model.load_weights('model_gemstones.h5')` to reuse it in other models."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model_gemstones.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# IV. Evaluate on testing folder\n\n## 1. Get samples from test folder\nCreate test data generator using class `ImageDataGenerator` and validate it providing the test directory `'/kaggle/input/gemstones-images/test/'`"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dir = '/kaggle/input/gemstones-images/test/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create `Test_Imgs` and `Test_Lbls` absolutely the same as we did with training folder. Convert them to numpy arrays - there are 358 images for test. `Test_Lbls` array will help to check is the model predictions are correct."},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Imgs, Test_Lbls = read_imgs_lbls(test_dir)\nTest_Lbls = get_class_index(Test_Lbls)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Crop test images"},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Imgs = crop_images(Test_Imgs)\nprint('shape of images in test set: {} '.format(Test_Imgs.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Plot test images and model predictions\nPlot image from test folder with a label, class which model predicted, and actual class."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f,ax = plt.subplots(5,5) \nf.subplots_adjust(0,0,2,2)\nfor i in range(0,5,1):\n    for j in range(0,5,1):\n        rnd_number = randint(0,len(Test_Imgs))\n        pred_image = np.array([Test_Imgs[rnd_number]])\n        pred_class = model.predict_classes(pred_image)[0]\n        pred_prob = model.predict(pred_image).reshape(87)\n        act = CLASSES[Test_Lbls[rnd_number]]\n        ax[i,j].imshow(Test_Imgs[rnd_number])\n        ax[i,j].imshow(pred_image[0])\n        if(CLASSES[pred_class] != CLASSES[Test_Lbls[rnd_number]]):\n            t = '{} [{}]'.format(CLASSES[pred_class], CLASSES[Test_Lbls[rnd_number]])\n            ax[i,j].set_title(t, fontdict={'color': 'darkred'})\n        else:\n            t = '[OK] {}'.format(CLASSES[pred_class]) \n            ax[i,j].set_title(t)\n        ax[i,j].axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nThe model tries! Finally it understands the color: some gemstones are really similar.  \n\n\n![Diamonds? really?](https://cdn.leibish.com/media/mediabank/blue-diamond-scale_1634.c8fbb.jpg)\n\n\n### **Please upvote this kernel if you find it useful** ðŸ™‹  \nFeel free to give any suggestions to improve my code."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}